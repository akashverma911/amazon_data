1.Coverted the given file to dataframe
2.Used beautifulsoup to fetch data from the urls.
3.Used Try and Catch to fetch data from different classes
4.Stored all data in 'data' list
5.Calculated time taken for every 100 urls.
5.Converted data list to json and printed it
6.Coverted timetaken list to json and printed it

My github link - https://github.com/akashverma911/amazon_data
colab linked in github
